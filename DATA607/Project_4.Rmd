---
title: "Document Classification"
date: "29 October 2018"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r init, echo=FALSE, message=FALSE}
library(dplyr)
library(rmdformats)
library(kableExtra)

# A Prefix nulling hook.
# Taken from https://stackoverflow.com/questions/22524822/how-can-i-remove-the-prefix-index-indicator-1-in-knitr-output

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set(output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if (is.na(comment)) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if (can_null && do_null) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
    # I added this line
    x <- gsub( "##", "", x)
  }

  default_output_hook( x, options )
})

knitr::opts_template$set("kill_prefix" = list(comment = NA, null_prefix = TRUE))
```

## Description

This project seeks to build a classifier that can predict if an email is spam or ham.  It will be using the Apache Spam Assassin public corpus as the dataset.

### Data Acquisition

The first step is to download the files.  I am selecting the most recent releases of spam and easy ham.

```{r file_download}
if(!file.exists("data/20050311_spam_2.tar.bz2") && !file.exists("data/20050311_spam_2.tar")){
  download.file("https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2", "data/20050311_spam_2.tar.bz2")
}

if(!file.exists("data/20030228_easy_ham_2.tar.bz2") & !file.exists("data/20030228_easy_ham_2.tar")){
  download.file("https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2", "data/20030228_easy_ham_2.tar.bz2")
}
```

Now that the files have been acquired, they need to be decompressed.

```{r decompress_files}
if(!dir.exists("data/spam_2")){
  R.utils::bunzip2("data/20050311_spam_2.tar.bz2", "data/20050311_spam_2.tar")
  untar("data/20050311_spam_2.tar", exdir = "data")
  # Delete the cmds file
  unlink("data/spam_2/cmds")
}

if(!dir.exists("data/easy_ham_2")){
  R.utils::bunzip2("data/20030228_easy_ham_2.tar.bz2", "data/20030228_easy_ham_2.tar")
  untar("data/20030228_easy_ham_2.tar", exdir = "data")
  # Delete the cmds file
  unlink("data/easy_ham_2/cmds")
}
```

## Processing Pipeline

### Train/Test Split

First we are going to split the data into training and testing sets using an 80:20 split. 

```{r}
set.seed(12345)
training_percent <- 0.8

train_test_split <- function(data, training_percent){
  n <- round(length(data)*training_percent, digits = 0)
  train <- sort(sample(data, n, replace = FALSE))
  test <- data[!data %in% train]
  return(list(test=test, train=train))
}

spam <- train_test_split(list.files("data/spam_2"), training_percent)
ham <- train_test_split(list.files("data/easy_ham_2"), training_percent)
```

The following table summarizes the number of messages included in each set:

##### Table 1. Spam and Ham Message Counts by Data Set
|      |       Training       |        Testing      |
|------|----------------------|---------------------|
| Spam |`r length(spam$train)`|`r length(spam$test)`|
| Ham  |`r length(ham$train)` |`r length(ham$test)` |

### Pull out Email Contents

This model will classify only on the basis of the content.  The header information will not be taken into consideration.  In order to illustrate the process I will take the first message in the spam training set as an example.  Here's what the file looks like:

##### Table 2. Preview of Spam Email Message
```{r spam_preview, null_prefix=TRUE, echo=FALSE}
example_file <- spam$train[1]
full_path <- paste0("data/spam_2/", example_file)
con <- file(full_path)
print(readLines(con), quote = FALSE)
```

The message is split into two parts: the header and the body of the message.  Since we want to analyze the body of the email one observes it is seperated from the header by a blank line.  This will be how we will determine where the body of the message begins.

### Approach

In order to process a message body I will remove all punctuation

### Remove Punctuation

```{r get_message, message=FALSE}
library(dplyr)
library(tidytext)

stem_hunspell <- function(term) {
  # Taken from https://cran.r-project.org/web/packages/corpus/vignettes/stemmer.html
  # look up the term in the dictionary
  stems <- hunspell::hunspell_stem(term)[[1]]
  if (length(stems) == 0) { # if there are no stems, use the original term
    stem <- term
  } else { # if there are multiple stems, use the last one
    stem <- stems[[length(stems)]]
  }
  stem
}

process_message <- function(full_path){
  message <- ''
  n_lines <- 0
  header_finished <- FALSE
  # Open the file
  con <- file(full_path, blocking = FALSE)
  # Go line by line
  for(line in readLines(con)){
    # Order matters.  I don't want to lead with the blank line between the header
    if(header_finished){
      n_lines <- n_lines + 1
      message <- paste(message, line)
    }
    # Look for the end of the header
    if(line == ""){
      header_finished <- TRUE
    }
  }
  # Close the connection to the file
  close(con)
  
  # Change text to lower case 
  n_characters <- nchar(message)
  lower_message <- tolower(message)
  n_capital_letters <- adist(message, lower_message)[[1]]
  # Remove all punctuation
  clean_message <- gsub("[[:punct:][:blank:]]+", " ", lower_message)
  n_punctuation_marks <- adist(lower_message, clean_message)[[1]]
  # Tokenize message
  words <- tm::scan_tokenizer(clean_message)
  # Remove stop words
  #stop_words <- tm::stopwords()
  #words <- words[!words %in% stop_words]
  # Stem words using hunspell
  #words <- unlist(lapply(words, stem_hunspell))
  
  return(list(words = words, characters = n_characters, capital_letters = n_capital_letters, punctuation_marks = n_punctuation_marks, lines = n_lines))
}

full_path <- paste0("data/spam_2/", example_file)

results <- process_message(full_path)
#df <- data.frame(word = bag_of_words, count = c(1)) %>%
#  group_by(word) %>%
#  summarise(count = sum(count)) %>%
#  ungroup() %>%
#  mutate(file = example_file, type = "spam")
```


## TF-IDF Normalization
